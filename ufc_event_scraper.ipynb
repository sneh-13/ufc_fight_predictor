{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "import_libs",
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import pandas as pd\n",
                "from bs4 import BeautifulSoup\n",
                "import time\n",
                "import re\n",
                "from datetime import datetime"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helper_functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_soup(url, max_retries=3):\n",
                "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
                "    for attempt in range(max_retries):\n",
                "        try:\n",
                "            response = requests.get(url, headers=headers, timeout=10)\n",
                "            if response.status_code == 200:\n",
                "                return BeautifulSoup(response.content, 'html.parser')\n",
                "            else:\n",
                "                print(f\"Warning: Status code {response.status_code} for {url}\")\n",
                "                time.sleep(2)\n",
                "        except Exception as e:\n",
                "            print(f\"Error fetching {url}: {e}\")\n",
                "            if attempt < max_retries - 1:\n",
                "                time.sleep(2)\n",
                "    return None\n",
                "\n",
                "def clean_text(text):\n",
                "    if text:\n",
                "        return re.sub(r'\\s+', ' ', text.strip())\n",
                "    return ''"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "scrape_events",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_all_events():\n",
                "    \"\"\"Scrape all completed UFC events.\"\"\"\n",
                "    url = \"http://ufcstats.com/statistics/events/completed?page=all\"\n",
                "    print(f\"Scraping events from {url}...\")\n",
                "    \n",
                "    soup = get_soup(url)\n",
                "    if not soup:\n",
                "        print(\"Failed to retrieve events page.\")\n",
                "        return []\n",
                "        \n",
                "    events_data = []\n",
                "    \n",
                "    # Find the events table\n",
                "    table = soup.find('table', class_='b-statistics__table-events')\n",
                "    if not table:\n",
                "        print(\"Could not find events table.\")\n",
                "        return []\n",
                "        \n",
                "    # Rows (skip header)\n",
                "    rows = table.find_all('tr')[1:]\n",
                "    \n",
                "    for row in rows:\n",
                "        cols = row.find_all('td')\n",
                "        if len(cols) < 2:\n",
                "            continue\n",
                "            \n",
                "        # Column 0: Event Name and Date (Date is usually in a span or secondary text)\n",
                "        # Actually, looking at the structure:\n",
                "        # Col 0: Link to event (Name) + Date below it\n",
                "        # Col 1: Location\n",
                "        \n",
                "        # Let's inspect Col 0 more closely\n",
                "        col0 = cols[0]\n",
                "        link = col0.find('a')\n",
                "        date_span = col0.find('span', class_='b-statistics__date')\n",
                "        \n",
                "        if not link:\n",
                "            continue\n",
                "            \n",
                "        event_name = clean_text(link.text)\n",
                "        event_url = link['href']\n",
                "        event_id = event_url.split('/')[-1]\n",
                "        \n",
                "        date_str = clean_text(date_span.text) if date_span else None\n",
                "        \n",
                "        # Col 1: Location\n",
                "        location = clean_text(cols[1].text)\n",
                "        \n",
                "        # Convert date to YYYY-MM-DD\n",
                "        formatted_date = None\n",
                "        if date_str:\n",
                "            try:\n",
                "                # Format is usually \"Month DD, YYYY\" e.g. \"November 11, 2023\"\n",
                "                dt = datetime.strptime(date_str, \"%B %d, %Y\")\n",
                "                formatted_date = dt.strftime(\"%Y-%m-%d\")\n",
                "            except ValueError:\n",
                "                formatted_date = date_str # Keep original if parse fails\n",
                "        \n",
                "        events_data.append({\n",
                "            'event_id': event_id,\n",
                "            'event_name': event_name,\n",
                "            'date': formatted_date,\n",
                "            'location': location\n",
                "        })\n",
                "        \n",
                "    print(f\"Found {len(events_data)} events.\")\n",
                "    return events_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "main_execution",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the scraper\n",
                "events = get_all_events()\n",
                "\n",
                "# Create DataFrame\n",
                "events_df = pd.DataFrame(events)\n",
                "\n",
                "# Save to CSV\n",
                "events_df.to_csv('ufc_events.csv', index=False)\n",
                "print(\"Saved to ufc_events.csv\")\n",
                "\n",
                "# Display\n",
                "display(events_df.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}